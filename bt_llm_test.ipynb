import os
from dotenv import load_dotenv
from pymongo import MongoClient
from sshtunnel import SSHTunnelForwarder
import json

 # Load environment variables from .envÏÏ
load_dotenv()

# Load SSH and MongoDB credentials
ssh_host = os.getenv("SSH_HOST")
ssh_port = int(os.getenv("SSH_PORT", 22))  # Default to port 22 if not specified
ssh_username = os.getenv("SSH_USERNAME", "")
ssh_password = os.getenv("SSH_PASSWORD", "")
remote_mongo_host = os.getenv("REMOTE_MONGO_HOST")
remote_mongo_port = int(os.getenv("REMOTE_MONGO_PORT"))
local_bind_port = int(os.getenv("LOCAL_BIND_PORT"))
mongo_database = os.getenv("MONGO_DATABASE")
mongo_collections = os.getenv("MONGO_COLLECTIONS").split(",")
mongo_username = os.getenv("MONGO_USERNAME")
mongo_password = os.getenv("MONGO_PASSWORD")
auth_db = os.getenv("AUTH_DB")  # Typically "admin" or the database where the user was created

#%%
import json
from bson import ObjectId

# Custom JSON encoder to handle ObjectId
class JSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, ObjectId):
            return str(obj)
        return super(JSONEncoder, self).default(obj)

try:
    # Set up SSH Tunnel
    with SSHTunnelForwarder(
        (ssh_host, ssh_port),
        ssh_username=ssh_username,
        ssh_password=ssh_password,
        remote_bind_address=(remote_mongo_host, remote_mongo_port),
        local_bind_address=("localhost", local_bind_port),
    ) as tunnel:
        print("SSH tunnel established.")

        # MongoDB connection
        mongo_uri = f"mongodb://{mongo_username}:{mongo_password}@localhost:{local_bind_port}/{mongo_database}"
        client = MongoClient(mongo_uri)

        # Access the database
        db = client[mongo_database]

        extracted_data = {}

        # Extract documents from each collection
        for collection_name in mongo_collections:
            collection = db[collection_name]
            documents = list(collection.find({}))  # Retrieve all documents
            print(f"Extracted {len(documents)} documents from collection '{collection_name}'.")
            # Save to a file (if configured)
            if collection_name in output_files:
                output_file = output_files[collection_name]
                with open(output_file, "w", encoding="utf-8") as f:
                    json.dump(documents, f, ensure_ascii=False, indent=2, cls=JSONEncoder)
                print(f"Data saved to {output_file}")

            # Store extracted data in a dictionary for further processing
            extracted_data[collection_name] = documents

except Exception as e:
    print(f"Error: {e}")
finally:
    print("Script finished.")

#%%
def prepare_bundestagsprotokolle(protokolle):
    data = []
    for protokoll in protokolle:
        datum = protokoll["datum"]
        wahlperiode = protokoll["wahlperiode"]
        sitzungsnummer = protokoll["sitzungsnummer"]
        for sitzungsverlauf in protokoll["sitzungsverlauf"]:
            for rede in sitzungsverlauf["rede"]:
                redner_id = rede["redner_id"]
                text = rede["text"]
                data.append({
                    "instruction": f"Was wurde in der Rede von Redner {redner_id} am {datum} gesagt?",
                    "input": f"Die Rede wurde während der Sitzung {sitzungsnummer} in der Wahlperiode {wahlperiode} gehalten.",
                    "output": text
                })
    return data

#%%
def prepare_stammdaten(abgeordnete):
    data = []
    for abgeordneter in abgeordnete:
        vorname = abgeordneter["vorname"]
        nachname = abgeordneter["nachname"]
        biographie = abgeordneter["biographie"]
        beruf = biographie["beruf"]
        for wp in abgeordneter["wahlperiode"]:
            wahlperiode = wp["wahlperiode"]
            data.append({
                "instruction": f"Welche beruflichen Hintergründe hat {vorname} {nachname}?",
                "input": f"Abgeordneter in der Wahlperiode {wahlperiode}.",
                "output": beruf
            })
    return data

#%% md
### Daten einlesen
#%%
# Dateien einlesen
with open("bundestagsprotokolle.json", "r", encoding="utf-8") as f:
    bundestagsprotokolle = json.load(f)

with open("abgeordneten_stammdaten.json", "r", encoding="utf-8") as f:
    abgeordneten_stammdaten = json.load(f)
#%% md
### Daten vorbereiten
#%%
# Funktionen aufrufen
protokoll_data = prepare_bundestagsprotokolle(bundestagsprotokolle)
abgeordneten_data = prepare_stammdaten(abgeordneten_stammdaten)

# Daten zusammenführen
fine_tuning_data = protokoll_data + abgeordneten_data

# Ergebnis speichern
with open("fine_tuning_data.json", "w", encoding="utf-8") as f:
    json.dump(fine_tuning_data, f, ensure_ascii=False, indent=2)

print("Fine-Tuning-Daten wurden erfolgreich erstellt und in 'fine_tuning_data.json' gespeichert.")

#%% md
## LLM Fine-Tuning
#%% md

#%%
from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments
from datasets import load_dataset
import torch

# Authenticate if necessary (for private models)
# Use `huggingface-cli login` in your terminal to log in and download the model locally.

# Correct model identifier (replace with actual model if necessary)
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

# Verify if MPS is available
if not torch.backends.mps.is_available():
    raise EnvironmentError("MPS (Metal Performance Shaders) ist auf diesem Gerät nicht verfügbar.")

# Set device
device = torch.device("mps")

# Load data
data_path = "fine_tuning_data.json"  # Ensure this file exists
dataset = load_dataset("json", data_files=data_path)["train"]

# Load tokenizer and model
tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=False)
model = LlamaForCausalLM.from_pretrained(model_id)

# Move model to MPS device
model.to(device)

# Preprocess function
def preprocess_function(examples):
    inputs = [
        f"{instruction}\n{input_text}" if input_text else instruction
        for instruction, input_text in zip(examples["instruction"], examples["input"])
    ]
    return tokenizer(inputs, padding="max_length", truncation=True, max_length=512)

# Split dataset
dataset = dataset.train_test_split(test_size=0.1)
train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# Tokenize datasets
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=5e-5,
    num_train_epochs=3,
    save_steps=10,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=2,
    fp16=False,  # Disable mixed precision for MPS
    report_to="none",
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
)

# Fine-tune the model
trainer.train()

# Save fine-tuned model
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")
#%% md
## Testing the Bundestags LLM
#%%
from transformers import pipeline

# Feinabgestimmtes Modell laden
pipe = pipeline("text-generation", model="./fine_tuned_model", tokenizer=tokenizer, device="mps")
test_input = "Was wurde in der Rede von Redner 123 gesagt?"
output = pipe(test_input, max_length=100, num_return_sequences=1)
print(output[0]["generated_text"])
